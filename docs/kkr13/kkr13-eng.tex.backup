%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% This file is delivered as an usage example of the file 'kkr13-eng.sty'.
% The style file 'kkr13-eng.sty' has been prepared for papers intended 
% to be presented at the XIII National Conference on Robotics in Poland.
%
% The file 'kkr13-eng.sty' contains definitions of macros for English
% and Polish authors. Thus their names are in English and Polish respectively.
% They define the same operations. English and Polish names are used
% only for authors' convenience. 
% Therefore both types of macros can be used in this type of the document.
% The same type of macros are defined in the style file defined for papers 
% written in Polish.
% Below there is a list of macros names in English and their equivalent forms
% in Polish.
%
%       \articleTitle          =   \tytulArtykulu
%       \titleFootNote         =   \stopkaPrzypisTytulu
%       \articleAuthor         =   \autorArtykulu
%       \affiliation           =   \instytucja
%       \theSameAffiliationAs  =   \instytucjaTaSamaJak
%       \articleShortTitle     =   \naglowekTytulSkrocony
%       \breakAuthorsLine      =   \zlamLinieAutorow
%       \authorsForHeader      =   \naglowekAutorzyArtykulu
%       \authorsCustomizedList =   \wlasnaListaAutorow
%       \abstract              =   \streszczenie
%       \Equation              =   \wzor
%       \refeq                 =   \refwzor
%       \reffig                =   \refrys
%       \refFig                =   \refRys
%
% Author: Bogdan Kreczmer 
% Last modification:      2013.12.09
% 
% In the case of any doubt, please, contact via e-mail.
% The address of the author is:
%  bogdan.kreczmer@pwr.wroc.pl
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\documentclass[11pt,twoside]{article}
\usepackage{kkr13-eng}
\usepackage{todonotes}
\usepackage[hidelinks]{hyperref}
\usepackage{array}
\usepackage{booktabs}
\newcommand{\superscript}[1]{\ensuremath{^{\textrm{#1}}}}
\newcommand{\subscript}[1]{\ensuremath{_{\textrm{#1}}}}

\articleTitle{3D Object Classification \\ Based on RGBD Images}

%
% For information of a project (if any)
%
\titleFootNote{The work described in this paper was conducted within author's Bachelor's dissertation}

%
% Parameters:
%   1 - initials of the first and middle name of an author
%   2 - the first and middle name of an author
%   3 - the last name of an author
%
\articleAuthor{A.}{Adam}{Kosiorek}

\affiliation[institute1]{ Institute of Automation and Robotics, The Faculty of Mechatronics of Warsaw University of\\ Technology, email: iair@mchtr.pw.edu.pl, website: http://iair.mchtr.pw.edu.pl}


\begin{document}


\listoftodos

\abstract{ 

  This paper introduces a Bag of Words (BoW) semantic object classification framework based on RGBD data registered by a Microsoft Kinect camera. The BoW concept in natural language processing is introduced and a BoW image processing pipeline is described. Each part of the pipeline (region detection, feature extraction and vector quantization) is characterised and the most successful algorithms are outlined. Applications of BoW in image processing and their results are discussed. Additionally, the problem of classification is described. Algorithms with high performance potential were identified and evaluated. Salient region detection is done by SIFT, ISS3D and Uniform Sampling. Description is performed by FPFH, PFH and PFHRGB. Vector quantization was carried out by the KMeans algorithm. An SVM with the RBF kernel trained with an All-vs-All was used as a classifier. We achieved the accuracy of 65.22\% on 8 categories of the Berkely 3D Object Dataset and 62.3\% on a 10 category dataset compiled by Zhang from the Tokyo University.
}

\maketitle

\section{ INTRODUCTION }

  It has never been easier to capture visual information. Popular storage services grow lighting fast due to terabytes of photographs and movies uploaded every day. The growth is so fast that hand-tagging and description, the traditional means of annotation, cease to suffice. They are ambiguous, emotional and rarely optimal. With no better solution at hand databases are becoming increasingly harder to browse. Another, but quite similar setting occurs in the field of mobile robotics. A mobile robot is there to interact with its environment. If so, it would be desirable for the robot to know what kind of surroundings it is in. The problem can be treated as a scene or object classification problem, one of the most popular computer vision issues in the last decade. \todo[inline]{ref}

  This paper tackles two issues. Firstly, the Bag of Words (or BoW for short) approach to image classification is reviewed. The Bag of Words method originated from the natural language processing domain \cite{tsai2012bag}. It makes a strong assumption that word occurrence in a text document define its meaning. It were Csurka \textit{et al} who first used BoW for image classification in 2004 \cite{csurka2004visual}. Since images cannot be simply casted into natural language documents a pipeline that help us achieve this goal is needed. It usually consist of: keypoint detection, feature extraction, vector quantization, together with codebook construction, and classification, steps that will be discussed later.

  Being efficient, efective and flexible, a Bag of Words processing pipeline is, however, a blessing in a disguise. While in fact it can provide satisfactory results with little effort, it is cumbersome to tune, thus spectacular performance is hard to achieve. One reason is the abundance of suitable algorithms available. Wide variety of building blocks might make it flexible, but it renders choosing an optimal combination infeasible.

  The rest of this work is organised as follows: Section 2 describes the Bag of Words technique in detail. Construction of an image model is discussed and the most popular algorithms are outlined. Basic information on a problem of classification are introduced the most popular approaches of solving it are discussed. Section 3 describes two renown RGBD image datasets suitable for evaluation of RGBD data-based object classifiers. Section 4 addresses the topics of experimental setup, conducted experiments and provides the results. Finally, Section 5 concludes this work.

\section{ BAG OF WORDS }

  Bag of Words is an intermediate representation in a form of a histogram used in natural language processing, information retrieval and data mining. It obliterates any grammatical information as well as word order in order to perserve word incidence only.

  \begin{figure}[!ht]
  \centering
  \includegraphics[width=0.7\textwidth]{../figs/bow_example}
  \caption{Bag of Words histogram}
  \label{fig:bow_example}
  \end{figure}

  The figure \ref{fig:bow_example} shows a piece of text and its BoW model, where articles and punctuation marks were left out. In order to compare different documents a global dictionary must be built. The dictionary (codebook) is constructed by taking every word from all available documents and removing duplicates. One can image that if a dictionary is of any considerable size resulting representation of especially small documents will be very sparse. Classification algorithms can be optimised for sparse data in order to boost performance.

  In natural language processing BoW is used to infer semantic meaning of documents. If we could translate an image into a text document we might be able to employ similar methods. A question arises: How does one make a text document from an image?

  \subsection{Processing Pipeline}	
    Tsai describes a well established pipeline that allows translation of images into text documents \cite{tsai2012bag}. It consists of the following steps: (1) region detection, (2) feature extraction, (3) vector quantization and (4) BoW histogram formation as can be seen in the figure \ref{fig:bow_pipeline}. The author summarises the most commonly used methods for performing these tasks. 

    \begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\textwidth]{../figs/tsai2012}
    \caption{Bag of Words pipeline. The figure comes from \cite{tsai2012bag}}
    \label{fig:bow_pipeline}
    \end{figure}
	
  \subsubsection{Region Detection}
    Characteristic region detection is the first step in any Bag of Words framework. Numerous detection methods have been developed, but choosing the right one for any particular case might prove tricky. 
    
    The most common detectors make use of a Harris corner detector or image's first or second derivatives. A Harris-Laplace detector is an example of a Harris-based detector. The Harris function is scale adapted and its outcome is a subject to a Laplacian-of-Gaussian (LoG) operator, which selects relevant points in scale space. Images' regions' 2\textsuperscript{nd} derivatives, namely the regions' Hessians, can be combined with a LoG operator as well. This combination allows selection of points significant in the two spaces: the scale space and the Hessian's determinant space. The latter entails the speed at which pixel intensities change in the neighbourhood of a point.	

    A number of more advanced recipes for salient region localisation have been developed and implemented. These include for example SIFT \cite{sift_keypoint}, SUSAN \cite{susan_keypoint} and Intrinistic Shape Signatures \cite{iss_keypoint}. The majority of keypoint detection formulas is being developed for the 2D domain. A number of them have been adapted to 3D, however. A comparative evaluation of detection algorithms available in the PointCloud Library (PCL, discussed below) can be find in \cite{pcl_keypoint_comparision}. Another comprehensive study is \cite{3d_keypoint_eval}. All these formulas, called sparse feature detectors, resort to selection of maxima in specific state spaces. An entirely different scheme is to use a dense feature detector. That is, take a uniformly sampled grid of points. Dense detectors have an advantage in that they sample slow changing regions in terms of gradient or hessian of an image. Examples of such regions would be a clear sky or a calm ocean. Li \emph{et al} showed that dense detectors generally outperform sparse ones \cite{fei2005bayesian}.
    
  \subsubsection{Feature Extraction}
    Suppose only coordinates of a keypoint were known. In case of even the smallest rotation or translation they would change and the keypoint would be lost. Keypoints should be described in a way that makes them invariant to affine transformations, changes in light intensity or colour saturation. It is hard to achieve all of these properties simultaneously, but methods that meet some of them exist.
    
    Keypoint description takes form of coordinates in a high-dimensional space. One of the most precise and repeatable algorithm is SIFT \cite{sift_features}, which is a 3D histogram of gradients structured as a 128-dimensional vector of floating point values. It is the most often extracted descriptor in BoW pipelines. Other methods include various colour descriptors, binary descriptors such as 512-dimensional GIST \cite{ponce2011cv}. There are techniques designed for 3D exclusively. Among them are Persistent Point Feature Histogram (PFH) \cite{pfh_rusu2008}, its faster alternative FPFH \cite{fpfh_rusu2009} and PFHRGB, which takes into account colour information, all implemented in PCL.
    
  \subsubsection{Vector Quantization}
    When features are extracted they have to be normalised. Vector quantization step have two main phases. The first one is a codebook construction from a training dataset. The second phase is responsible for parsing (or translation) raw image descriptors into a form compatible with the newly constructed codebook. The simplest way of building a dictionary is to find patterns or regions within the descriptors computed on a training dataset. Then, the parsing phase is about assigning a descriptor to one of the codebook elements (e.g. to one of the regions). Suppose we use centroids or medoids of the discovered regions as our codebook's entries. Then, we can match a descriptor to one of the centroids performing a nearest neighbour search. The descriptor parsed this way is called a \textit{visual word}. Finally, we build a histogram of visual word occurrence. The histogram have a fixed size equal to the codebook size. Each bin of the histogram tells us how many visual words of a particular kind were in the description of the corresponding image.
    
    The kMeans is the single most popular vector quantization algorithm used in the BoW pipelines \cite{tsai2012bag}. Developed in 1950's, it is well known and simple to implement. kMeans divides all data into a predefined amount of clusters and computes the clusters' centroids. Many modifications and alternative versions have emerged \cite{kmeans_jain2010data}. Some of them are: faster than the original \emph{approximate kmeans}, \emph{hierarchical kmeans}, which automatically chooses the final number of clusters and a \emph{soft kmeans} --- a variation of the algorithm that allows a fuzzy alignment. The fuzzy alignment mean that each point can belong to several clusters with different weights. A weight can be proportional for instance to the inverse square of the distance to a centroid. 
    
    If computational cost is of no concern, or if required precision is of the utmost priority, a Gaussian Mixture Model (GMM) can be used. The GMM partitions data into a set of clusters, finds their means and covariances. In the parsing step we compute a probability distribution of a descriptor over all the clusters. We can then build a fuzzy aligned histogram taking the probability distribution as weights. The GMM can be thought of as a generalisation of the kMeans algorithm. The drawback of the GMM is its massive computational cost in comparison with still expensive kMeans. Recently, Perronnin \textit{et al} proposed GMM based fisher-kernel vector quantization step with superior results \cite{fisher1, fisher2}.		
    
    After we choose a vector quantization algorithm, we have to decide what size of a dictionary to use. In a simplest case, when the kMeans is used, we can treat each clusters' centroid as a visual word. Therefore, the dictionary size defines how much information we retain in a BoW histogram. A big training set containing many classes with large inter-class and inter-class variance is likely to require a huge codebook. On the other hand, too big a dictionary might introduce quantization artefacts. One has to bear in mind that the vector quantization step is the most time-consuming part of the BoW pipeline. The highest computational complexity is associated with the codebook construction step and it is$O(n^3)$, where $n$ is the codebook size.		
    
    It is possible to combine several feature extraction algorithms before creating the codebook (Early Fusion) or create many codebooks and concatenate resulting histograms (Late Fusion). Both schemes are as simple as vector concatenation. The Early Fusion concatenates two feature vectors if they have been extracted from the same keypoint. The Late Fusion joins histograms, outputted by separate quantization steps.

  \subsection{Applications in Computer Vision}
    The Bag of Words approach is a tool than enables us to represent an image by a single feature vector of a chosen size. Its most popular applications in computer vision are Content Based Image Retrieval (CBIR) and Scene/Object Classification.

  \subsubsection{Content Based Image Retrieval}
    CBIR is a computer vision approach to image retrieval from large image databases. Tangelder \emph{et al} provides an overview of techniques applicable to 3D objects \cite{tangelder2008survey}. The task of image retrieval is to find a database entry that fulfil certain conditions. If we wanted to perform the task efficiently, we would have to meet the following criteria \cite{toldo2009bag}: (1) All entries should be indexed in a concise way, (2) a (dis)similarity measure should be provided and (3) an efficient search algorithm should be available. 
    
    Database entries (images) should be indexed beforehand. Otherwise we would have to compare all the entries explicitly. An index must be compact as well as discriminative. Suitable algorithms can be divided into three general categories: (1) feature based methods, (2) graph based methods and (3) other methods.

    Feature based methods can be further divided into global features and local features. The global features take form of a single vector (or a point in a \emph{d} dimensional space). They are usually associated with object's mass or volume or distributions of these. Being easy to compute and straightforward to implement, their discriminative power is rather low --- they cannot be used for partial matching, but are well suited for an early preprocessing. 
    
    Local features, on the other hand, describe object's characteristic regions. Their shape is similar to that of the global features, but instead of a single point in space there are multiple ones --- one for each considered region. Tangelder argues that local features based approaches are inefficient and lead to a complex indexing problem \cite{tangelder2008survey}. At the same time Toldo \emph{et al} and Li \emph{et al} show that Bag of Words local feature based approach has no such drawbacks. On the contrary --- it is easy to implement, efficient and provides state-of-the-art results.
    
    Feature based methods can be either local or global. Local features describe characteristic regions of an object, which have to be identified first. We have to carry out similar computation for each salient region. An outcome is a set of vectors, one vector for each region in \emph{d} dimensional space, where \emph{d} is dimensionality of the descriptor. Global features are generally easier (faster) to compute and more compact --- they take form of a single \emph{d} dimensional vector. Often global features are more computationally efficient and can be easily used for object indexing . Before the advent of BoW it was not clear how to use local features for indexing, nor there was any straight-forward dissimilarity measure available \cite{tangelder2008survey}. The Bag of Words approach solved these issues. What is more, it is easy to implement, efficient and provides state-of-the-art results \cite{li2010investigating}.
    
  \subsubsection{Scene and Object Classification}
    Scene and object classification are among the most popular issues of computer vision nowadays. We would like to perform those tasks automatically for the sake of surveillance, navigation or automatic image tagging. The very first attempt to use BoW in scene classification was made by Csurka \textit{et al} in 2004 \cite{csurka2004visual}. They were inspired by an idea of a ``texton'', a building block of texture introduced earlier in pattern recognition and texture classification. The authors suggested a BoW pipeline described above. Then, they fed the resulting BoW histograms into two classifiers: Support Vector Machine (SVM) and Na\`ve Bayes (both discussed below). Fei-Fei \emph{et al} refined the original approach by examining several keypoint detectors and descriptors \cite{fei2005bayesian}. Moreover, they developed a novel probabilistic graphical model for classification. They achieved accuracy of 76\% on a large 13 category dataset. 
    
    One of the best entries in an ILSVRC2013\footnote{\url{http://www.image-net.org/challenges/LSVRC/2013/results.php}} challenge, attempting to classify millions of images into 1000 categories, was submitted by a fisher vector based BoW approach as well. The top-5\footnote{top-5 --- the classification result is successful if the ground-truth category is in one of the five most probable categories predicted} accuracy rate was close to 85\%.
	      
\section{Problem of Classification}

    To classify means to, given a set of categories, produce a category label for a given set of features \cite{ponce2011cv}. Image classification fits this description perfectly. The only difficulty lies in complexity of a raw image. Suppose we have a $320$ by $240$ three channel colour image. This amounts to a total of $230400$ dimensions, far too many to feed into any classifier directly. Fortunately, a Bag of Words histogram is a compact and discriminative intermediate representation that solves this issue. It can be used with any classifier. Classifiers can be two-class classifiers (e.g. binary) or multi-class classifiers. The latter are often a combination of several binary classifiers. The most popular classifiers are: k-Nearest Neighbours, Logistic Regression, Softmax Regression, Na\`ive Bayes and Support Vector Machine.
    
    \subsubsection{Na\`ive Bayes}
    Assume that we have a \textit{n}-dimensional feature vector $\mathbf{X}$ such that $\mathbf{X} = \left\{x_1, x_2, ... , x_\mathit{n}\right\}$ and an unknown category label \textit{C}. The probability of the category \textit{C} given the feature vector \textbf{X} is $P\left(\mathit{C}\middle|\mathbf{X}\right) = P\left(\mathit{C}\middle|\left\{x_1, x_2, ..., x_\mathit{n}\right\}\right)$. The strong (or na\`ive) Bayes assumption says that features are independent of each other, that is $P(\mathbf{X}) = P(x_1)*P(x_2)*...*P(x_\mathit{n})$. If so, we can compute probability distributions of every class in our training set given every feature. Then, for any new feature vector the joint probability distribution over a set of features can be calculated and an appropriate class label can be assigned.
    
    \subsubsection{k-Nearest Neighbours (kNN)}
    The nearest neighbours classifier assumes that if there are labelled points in a neighbourhood of a newly added point, then the new point can be classified based on it's neighbours' labels. There is an issue of choosing the correct number of nearest neighbours (or k) to consider.	
    
    \subsubsection{Support Vector Machine (SVM)}
    \begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\textwidth]{../figs/svm}
    \caption{Classification by a Support Vector Machine. The distance of the separating plane from both datasets is maximal. The figure was originally published in \cite{ponce2011cv}}
    \label{fig:svm}
    \end{figure}
    
    Assume that a set of pairs $\{\{\mathbf{x}_1, y_1\}, \{\mathbf{x}_2, y_2\}, ...\}$ where $x_\mathit{i}$ are features and $y_\mathit{i} \in \{-1, 1\}$ are labels is given. Further assume that points with different labels are two linearly-separable datasets as shown in the figure \ref{fig:svm}, where dots and circles are points with different labels. Then, parameters \textbf{\textit{w}} and  \textit{b} exist such that	
    \begin{equation}	                        y_\mathit{i}\left(\mathbf{w}*\mathbf{x}_\mathit{i} + b\right) > 0
    \end{equation}
    for every $\mathbf{x}_\mathit{i}$ or a data point. This equation specifies constraints on a plane (or a hyperplane in general) that separates different classes. A Support Vector Machine finds the parameters \textbf{\textit{w}} and  \textit{b} so as to maximise the distance of the plane from members of both classes. We can combine several SVM classifiers in a 1-versus-all or an all-versus-all scheme.

\section{ DATASETS }

  Datasets are essential for the following reasons: (1) Supervised learning algorithms have ravenous apetite for labelled data. An open database of labelled RGBD images would be of a great help in evaluation of algorithms. (2) If any comparison of algorithms is to be meaningful it should be performed on the same data. There is a multitude of RGB datasets available: ImageNet\footnote{\url{http://www.image-net.org/}}, PascalVoc\footnote{\url{http://pascallin.ecs.soton.ac.uk/challenges/VOC/}}, LabelMe\footnote{\url{http://labelme.csail.mit.edu/Release3.0/browserTools/php/dataset.php}} and SUN\footnote{\url{http://groups.csail.mit.edu/vision/SUN/}}, among others. Unfortunately, they do not contain any depth information and cannot be used for the purpose of object classification based on RGBD data. On the other hand, the vast majority of RGBD datasets are focused on tracking or instance-level recognition, have insufficient number of examples per category or have too few categories. Author managed to find only two datasets suited to his needs. They are: the Berkeley 3D Object (B3DO) dataset \cite{B3DO} and a dataset compiled by Zhang \emph{et al} at the University of Tokyo\cite{zhangcategory}. The latter will be abbreviated Tokyo dataset from now on.

  \subsection{Berkeley 3D Object Dataset}
    \begin{figure}[!ht]
    \centering	
    \includegraphics[width=.75\textwidth]{../figs/b3do_objects}
    \caption{B3DO objects: a) cups b) bottles c) books d) keyboards}
    \label{fig:b3do_objects}
    \end{figure}    
    
    The Berkeley 3D Object dataset, shown in \refFig{fig:b3do_objects}, was specifically designed for the purpose of object detection and classification. It consists of cluttered images in indoor environment, with many labelled and random objects per image. The images are often poorly lit and objects are partially occluded. There are around 50 classes with more than 20 examples in each of them. RGBD data are provided as pairs of colour images (8 bit RGB jpeg files) and depth maps (16 bit png files). Images are densely labelled --- for every pair of images there is an xml file with annotations. Each annotation states a category of an object as well as it's bounding box location. Since neigher image segmentation nor object detection is addressed in this paper we had to extract objects from the images before the dataset could be used.

  \subsection{The University of Tokyo Dataset}	
    \begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{../figs/tokyo_horizontal}
    \caption{The Tokyo dataset: a) basket b) bicycle c) box d) bucket e) cart f) freezer g) notebook h) printer i) scanner j) scene}
    \label{fig:tokyo}
    \end{figure}

    The Tokyo dataset, shown in \refFig{fig:tokyo}, is comprised of high quality images of single labelled objects in different settings, with changing viewpoint, object orientation and texture. Additional unlabelled object can be present, however, and should not be taken into account. No bounding boxes are given, which renders the classification task similar to the scene classification problem. The dataset is presented as colour images (8 bit RGB jpeg files) and depth maps (csv files with XYZ coordinates of each pixel). This dataset is aimed at the task of object classification in casual images.

\section{ EXPERIMENTS }
  There is a plenty of algorithms suitable for any of the four steps of Bag of Words image processing. Most of them have multiple tunable parameters and their performance might be interdependant on each other. Comprehensive evaluation is inevitable if an optimal combination is to be selected. Considering the PointCloud Library\footnote{\url{http://pointclouds.org/}} (PCL) alone yields far too many options to test, with 12 keypoint detectors and more than 20 feature descriptors. A literature review helped to narrow the scope of evaluation considerably. All experiments were run on a notebook with Intel Core i7 quad core 2.2 GHz CPU, 8 GB of RAM and a nVidia M540 GPU. 

  \subsection{Detectors and descriptors}
    Keypoint detection algorithms were compared with respect to repeatability, invariance in \cite{pcl_keypoint_comparision} and in terms of object retrieval and recognition in \cite{3d_keypoint_eval}. 3D-SIFT and ISS both achieve the highest robustness, while the latter appears to be the best choice for object retrieval. Dense detector (or uniform sampling) achieves the best performance in 2D image classification \cite{tsai2012bag}. Therefore, this paper compares SIFT, ISS and Uniform Sampling keypoint detection techniques. Additionaly, PCL descriptors were evaluated in terms of performance in object retrieval in \cite{pcl_features}. PFHRGB and SHOTCOLOR delivered the best results, while their simpler counterparts (PFH, FPFH, SHOT) were only a little worse. USC, which is a global descriptor and thus not suitable for the Bag of Words approach, achieved very similar accuracy. The family of PFH descriptors is evaluated in this paper.
    
    \begin{table}[!hbtp]
    \centering
      \caption{Highest accuracy obtained with FPFH, PFH and PFHRGB descriptors on the B3DO dataset}
      \label{tab:desc_b3do}
     \begin{tabular}{*4c}
     \toprule
       Detector & ISS3D & SIFT & Uniform Sampling  \\ 
       Descriptor & \multicolumn{3}{c}{Accuracy {[\%]}} \\
     \midrule
       FPFH & 65.22 &  56.52 & 56.07  \\ 
       PFH & 59.32 &  54.83 & 59.50 \\
       PFHRGB & 63.35 & 52.34 & 53.27 \\
       
   \bottomrule
    \end{tabular}
    \end{table}

    The best results of each detector/descriptor combination on the B3DO dataset are shown in table \ref{tab:desc_b3do}. ISS consistently proved to be the most successful detection algorithm in this setting. SIFT and Uniform Sampling scored similar accuracy but the latter is significantly faster. SIFT is in fact the slowest of the three. As for the descriptors, FPFH delivered the best performance even though it is an approximate and theoretically the least precise algorithm. It is also the fastest of the three. It appears that the combination of ISS and FPFH is not only surprisingly good in terms of accuracy; it is also the fastest. These two algorithms allow near real time point-cloud processing and classification. PFHRGB and SIFT are several times slower.
      
  \subsection{Codebook}
    KMeans is used in almost every implementation of the Bag of Words image processing pipeline \cite{tsai2012bag, toldo2009bag}. Csurka \emph{et al} showed that the number of visual words has a significant impact on the final results \cite{csurka2004visual}. We run a number of tests to discover an optimal codebook size for our setting.

    \begin{figure}[!ht]
    \centering	
    \includegraphics[width=.75\textwidth]{../figs/clustering_centroids_b3do}
    \caption{Influence of dictionary size on the overall accuracy. B3DO dataset with ISS keypoint detector and FPFH feature descriptor}
    \label{fig:cluster_b3do}
    \end{figure}

    The results from the figure \ref{fig:cluster_b3do} partially confirm Csurka's findings. The performance raises with the increasing number of centroids up to the point of 1500 centroids and 65.22\%. The discrepancies might be caused by the following factors: (1) ISS detector finds too many or irrelevant points, thus introducing noise or (2) The FPFH descriptor has too few dimensions (33) to be divided into more than 1500 regions in a meaningful way. 


  \subsection{B3DO}
    The majority of the point clouds in the B3DO dataset contain several different objects. We have extracted every individual object from the clouds and divided them into 78 separate categories. Unfortunately, they were badly balanced \emph{i.e.} the number of objects in a category varies from 1 (tape) to 299 (table). We have picked 8 categories at random with a restriction that there should be at least 50 object instances in a category. Then, the objects were split into two sets (training and testing) with a proportion of 1:1. As the names suggest, they are used for estimation and evaluation respectively.

    The highest accuracy achieved for this dataset is 64\%. Table \ref{tab:b3do_conf_matrix} contains a confusion matrix, number of examples per category and accuracy. The confusion matrix depicts classification errors. Let m\subscript{i, j} be an element of the confusion matrix at the i\superscript{th} row and j\superscript{th} column. It shows how many elements from the i\superscript{th} category was assigned to the j\superscript{th} category. A high value of m\subscript{i, j} such that $i \neq j$ indicates that the classifier cannot distinguish between those two classes.

    \begin{table}[!ht]
    \centering
    \caption{Results on the B3DO dataset with ISS keypoint detector, FPFH features and a dictionary of 1500 words; \textbf{Average accuracy: 65.22\%}}
    \includegraphics[width=0.9\textwidth]{../figs/b3do_conf_matrix}	
    \label{tab:b3do_conf_matrix}
    \end{table}

   

    More than half of the bottles were assigned to the cup category and keyboards were often mistook for books. These two error types are easily explained by a high degree of similarity of objects (bottles and cups are usually round and tall, books and keyboards are flat and rectangular) Surprisingly, however, objects from half of the categories were frequently marked as cups. Some of these misclassification errors might be caused by very poor quality of images. Many of them are occluded poorly lit low resolution images.	

  \subsection{Tokyo}
    \begin{table}[!ht]
    \centering
    \caption{Results on the Tokyo dataset with ISS keypoint detector, PFH features and a dictionary of 3000 words; \textbf{Average accuracy: 62.30\%}}
    \includegraphics[width=1\textwidth]{../figs/tokyo_conf_matrix}	
    \label{tab:tokyo_conf_matrix}
    \end{table}

    Every of the 343 images from Tokyo dataset was used. Data was split into test and train set in proportions 1:2 in order to provide more training examples due to low number of objects in some classes.

    Even tough the overall accuracy achieved on Tokyo dataset is similar in value to that of the B3DO, the structure of the result is very different. It is clearly visible that there is a strong correlation between a per class accuracy and the number of entries in this class. The highest performance in the bicycle category is coupled with the largest number of entries. On the other end of the scale there are cart and printer categories with only 2 and 4 entries respectively. On top of that there are differences between some classes are marginal. The majority of carts and baskets were put into the bucket category. It does not surprise, for they are simply akin as can be seen in figure \ref{fig:tokyo} on the page \pageref{fig:tokyo}.

\section{ CONCLUSION }

  In this paper we have tackled the issue of three dimensional point cloud classification with a bag of words-based approach. The main focus was put on the design and efficient implementation of the pipeline in a way that allows easy addition of algorithms and switching them at run-time. What is more, we have gone to great lengths in order to find scientific databases that are suitable for the task of point cloud classification. We believe that the presence of such databases is fundamental, for it allows meaningful comparison with other researchers. We have experienced the following problems: Firstly, point cloud processing is computationally expensive and inefficient. Due to the lack of structure in point clouds (\textit{i.e.} no predefined relationship between points) additional calculations have to be performed (\textit{e.g.} finding nearest neighbours requires distance comparison for all points in a cloud, whereas for a 2D image one instantly knows where adjacent pixels lie). Secondly, there are virtually no databases that would allow extensive training of the bag of words-based pipeline. Machine learning algorithms have ravenous appetite for labelled data. As the newest findings in the state-of-the-art 2D object classification suggest, an order of tens of thousands of samples is a prerequisite for any satisfactory results We had to make do with as few as 330 objects in our training set. Moreover, there is very little interest in the scientific community for this particular task. The majority of research is conducted on retrieval of CAD-like objects from mesh databases. Last, but not least, there is no 3D detection or description algorithm designed solely for the purpose of bag of words object classification.

  In conclusion I would not recommend any bag-of-words based approach classification of real point clouds registered with a low quality camera such as Microsoft Kinect. Being computationally expensive and rather inefficient, it delivers inferior performance in terms of accuracy and speed when compared to 2D RGB image based algorithms. To makes matters worse, generalisation of 2D algorithms to the 3D domain can be very involved, causing the cutting-edge inventions of the 2D image processing scientists unavailable for the purpose of 3D point cloud processing.

\subsubsection{\small Mathematical formulae}

 The number of a formula has to be written
 in parentheses shifted to the right (see below). Symbols of variables 
 should be written in italic.
 \begin{equation}
   x = \frac{a^{(1-z)} + b}{10 - d_2}
 \end{equation}

 \noindent
 Intervals of values should be written without spaces, e.g.\ 4--45MPa.


\section{FIGURES AND TABLES}

\subsection{Figures and their arrangement}

 Figures should be numbered with the Arabic numerals (see fig. 
 \ref{fig-example}). 

 \begin{figure}[hbtp]
  \fbox{\rule{0mm}{45mm}\rule{85mm}{0mm}}
  \caption{The figure caption}
  \label{fig-example}
 \end{figure}

\subsection{Tables}

 Tables have to be formated in the way presented below (see table 
 \ref{tab-example}). The tables have to be numbered  separately from 
 the figures.

 \begin{table}[hbtp]
  \caption{The table caption}
   \label{tab-example}
   \begin{tabular}{|c|c|c|c|c|}\hline
       Column 1 & Column 2 & Column 3 &  Column 4 & Column 5\\ \hline
       $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$\\ \hline
    \end{tabular}
 \end{table}

\section{CITATIONS}

Citations of papers should be marked  with square brackets, e.g.
\cite{Ming}. The references have to be arranged in 
the alphabetic order relative to the authors' surnames (see the example
below).

\begin{thebibliography}{99}
\bibitem{Litwiniec}
    A. Litwin et al. Transport phenomena in InSb doped with various impurities.
    In: 11 International Conference on the Physics of Semiconductors.
   {\em Proceedings}. Warszawa -- Poland, July 25--29, 
    1972. Vol. 2, pp. 952--957.

 \bibitem{Lamadrid} 
  J.~G. de~Lamadrid.
  \newblock Avoidance of obstacles with unknown trajectories: Locally optimal
   paths and periodic sensor readings.
   \newblock {\em The Inter. Journal of Robotics Research}, 13(6):496--507,
   December 1994.

 \bibitem{Ming}
   A~Ming, K.~Kajihara, M.~Kajitani, and M.~Shimojo.
   \newblock Development of a rapid obstacle sensing system using sonar 
   ring for mobile robot.
   \newblock In {\em Proc. Int. Conf. on Robotics and Automation}, 
   volume~3, pp. 3068 -- 3073, Washington, D.C., May 2002.

 \bibitem{Singh:1987:Convex:Shapes}
   J.~Sanjiv Singh and Meghanad~D. Wagh.
   \newblock Robot path planning using intersecting convex shapes: Analysis and
   simulation.
   \newblock {\em IEEE Journal of Robotics and Automation}, RA-3(2):101 -- 108,
   April 1987.
\end{thebibliography}
\end{document}



