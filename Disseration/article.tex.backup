\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}

%opening
\title{Bag of Words 3D object categorization based on scale-invariant features}
\author{Adam Kosiorek}
\bibliographystyle{abbrv}
\begin{document}

\maketitle

\begin{abstract}

\end{abstract}
 

\section{Introduction}

  Mobile robotics has already begun its entrance into our households. It is conceivable that numerous tasks from our daily routines will be no
  longer of our interest, for personal robotic assistanst will take care of them. Among their most obvious duties are now or will be:
  cleaning, cooking  and elderly care. A robot aspiring to become a personal assistant has a number of requirements to fullfil. These 
  requirements might consist of, but are not limited to: an ability of simultaneous localization and mapping or SLAM, a clever human-machine 
  interface and a capability of interaction with its enviornment. As for the last part, a capacity of object categorization is essential. One 
  has to bear in mind that the categorization is very different from: recognition, content based image retrieval or CBIR and detection as outlined
  in \cite{csurka2004visual}. 
  
  An intermediate image representation that seems to yield the best results in scene classification, 2D object categorization and 3D shape retrieval
  over the past decade as seen in \cite{fei2005bayesian}, \cite{csurka2004visual}, \cite{li2010investigating} and \cite{tsai2012bag}
  is a Bag of Words model. It seems that it has not been used for 3D object categorization so far.
  
  Bag of Words model has numerous advantages over raw images. Firstly, with appropriate features used for word formation the resulting 
  representation can be invariant to affine transforms and lighting conditions. Moreover, it addresses the issue of a semantic gap. It is 
  challanging to determine whether certain low level visual features (or their description) are characteristic for any higher-level object.
  On the other hand, clustering thousands of such described features yields a relatively low number of intermediate level features - the visual words.
  
  A typicial pipeline for Bag of Words scene or object categorization is as follows:
  \begin{itemize}
  \item Points or regions of interest have to be detected.
  \item Each keypoint have to be described, thus resulting in a point in multi-dimensional space (or a vector)
  \item Every vector have to be assigned to a corresponding term from a pre-constructed visual vocabulary by a vector 
  quantization algorithm.
  \item The obtained histograms of visual words has to be fed into a multi-class classification algorithm.
  \end{itemize}
  
  Csurka \textit{et al} \cite{csurka2004visual} has presented a bag of words framework for 2D object categorization. They have employed the
  Harris affine detector for keypoint detection and SIFT for keypoint description. Even tough the training set has produced circa. 640 000 
  keypoints the visual vocabulary has been constructed by clustering with kmeans algorithm only 5 000 keypoints per category or 35 000 in 
  total. Then, histograms of visual words were processed by multi-class classificators - namely the Na\"{i}ve Bayes and the Support Vector Machine.
  As expected, the SVM proved superior yielding 85\% accuracy as opposed to Na\"{i}ve Bayes' 72\%. While being efficient and quite effective, this
  approach considers only two dimensional data. Mobile robots are usually equipped with a device capable of capturing three dimensional point clouds
  such as laser scanners or rgbd cameras. Utilization of the enriched spatial information might result in a higher accuracy.
  
  Recelty Zhang \textit{et al} \cite{zhangcategory} has employed a semi-supervised graphical model object categorization approach. 
  A structure-based graphical model is trained using the spatial-information from the RGBD images. It is believed that the spatial
  information can help overcome intra-category texture variations and various rigid transformations, thus enabling the user to provide
  just one annotated image of an object from every category to initialize the training. The model can then be refined with not annotated images.
  Furthermore, object is divided into several parts and a separate codebooks are constructed for each of them. The matching can be performed 
  on 2D or 3D images. Their average accuracy is above 85\% on 5 categories. Even though Zhang's method is easy to use for an end-user and 
  delievers state-of-the-art result it is very complex and difficult to implement.
  
 
  
\section{Literature review} 

\subsection{3D content based shape retrieval}

  Textual annotation and text-based search engines might provide statisfactory results when text documents are concerned. This method fails
  \cite{tangelder2008survey}, however, as soon as 2d images or 3d shapes are examined. One possible explanation is that human-annotations
  of visual (either 2d or 3d) objects is strongly dependant on the annotator's backgroud i.e. language, ethnicity. Content Based Image
  Retrieval, introduced in 1990s, lacks this disadvantage \cite{tangelder2008survey}. CBIR was first adapted to 2D images as a method of 
  extracting low-level visual features \cite{tsai2012bag}. A hypothesis is that the extracted features can be described, thus creating an 
  intermediate represenatation of an image. Such representations can be a subject to comparison with respect to some dissimilarity measure.
  Visual features are usually described as (key)points in a high-dimensional space \cite{tsai2012bag}. The dissimilarity measure can take a 
  form of a metric in this space. Therefore, distinct features can be compared in terms of a distance between them.

  A shape retrieval framework can be divided into two phases. Firstly, a database has to be built offline. A pre-annotated dataset is a
  prerequisite. Such a database should not contain images, nor shapes, but rather extracted and described keypoints of those. Secondly, 
  queries can be processed online by matching them against database's entries. A searching algorithm should be available. For a better 
  performance, the offline database should be organized in a structure that would reduce computational effort of finding the best match.
  A query can be formed in several ways: One can browse a database and select an entry as a query, feature description can be provided 
  or a query-by-example can be made. In the last case, feature descriptors has to be computed before the process of finding the best 
  match can begin.

  Shape matching techniques can be divided into three broad categories \cite{tangelder2008survey}, namely (1) feature based methods, (2) 
  graph based methods and (3) other methods.

\subsubsection{Feature based methods}

  Feature based methods focus on geometric features and their spatial distribution. This category can be further divided into (1) global features, 
  (2) global features distribution, (3) spatial features and (4) local features. The sub-category is very different, for it employs several 
  descriptors, one for each sub-region of an object. The first three method make use of just one descriptor per object. The descriptors are 
  \textit{d}-dimensional, with a \textit{d} being a fixed number of dimensions for all shapes. Only the local features approach provide satisfactory 
  results in partial matching.

\subsubsection{Graph based methods}

  Graph based methods try to utilize more information than feature based methods mentioned above. The latter base predominantly on the
  geometrical properties of an object. The former, on the other hand, build a graph reflecting relationships  between the geometrical
  properties. 


\subsubsection{Other methods}


\subsection{A bag of words approach}

  Even though the process of feature extraction and description is automatic, the CBIR has a major drawback commonly referred to
  as a semantic gap. Acording to \cite{tsai2012bag}, a semantic gap is „the gap between th extracted and indexed low-level features
  by computers and the high-level concepts (or semantics) of users' queries”. That is, for visual features extracted and concepts
  operated by humans are on very different levels of abstraction, a formulation of users' queries in terms of low-level features is
  highly problematic. 


\subsection{A bag of words approach for 3D object categorization}

\section{Knowledge base}

\subsection{Detection}

    The purpose of the detection process is to find interest points that would be distinctive to the given object and yet
    invariant to affine transforms as well as enviornmental conditions. The Scale-Invariant Feature Transform as described
    in \cite{lowe1999object} has been used extensively for the past decade and can boast of all the required characteristics.
    
    In order to compute the keypoints' locations the processed image is rescaled into multiple scales. To identify keypoints 
    locations the minima and maxima of a difference of Gaussian in the scale space are found. Each point is sampled relative to 
    its scale. Candidate object models are defined. Afterwards, a scale-space Hoguh transform hash-table is formed and then through
    a least-squares fit the final model estimation is carried out.

\subsection{Description}
\subsection{Vector Quantization}
\subsection{Prediction}

\subsection{OpenCV}
\subsection{PointCloud Library}

\section{Tagger3D}

\subsection{Design basis}
\subsection{Processing pipeline}
\subsection{System architecture}
\subsection{Implementation details}

\section{Testing enviornment}

\subsection{Dataset}
\subsection{Workstation}

\section{Results}

\section{Conclusions}


\bibliography{article}

\end{document}

