\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[top=3cm, bottom=6cm, left=4cm, right=3cm]{geometry}
\usepackage{cite}

\linespread{1.15}

%opening
\title{Object Categorization based on RGBD data}
\author{Adam Kosiorek}
\bibliographystyle{abbrv}
\begin{document}

\maketitle

\begin{abstract}

Abstrakt

\end{abstract}

%\include{introduction}

\section{Bag of Words image representation}

  As portreyed in \cite{tsai2012bag} the Bag of Words or BoW model has originated from the text retrieval domain. Originally the model enabled a vector-like representation of a text document. One of the most simple cases would be to, given a dictionary, construct a histogram depicting the incidence of words in a particular document. Such an approach obliterates any grammatical dependencies in order to retain only the statistical information associated with each word. It is believed that the words' frequencies are connceted to the semantic meaning of the document. The following questions emerge: 
  
  \begin{itemize}
    \item How does this apply to the area of computer vision?
    \item How can one convert an image to a text document? 
  \end{itemize}
  
  In order to answer the first question one has to consider how computers process data. Digital images are nothing but streams of binary code. A simple conversion to an RGB format makes the data structured and is enough for people to understand pictures' content. Unforunately, such image representation is extrmelly vulnerable to translation and rotation as well as changing lighting conditions. One way to address these disadvantages would be to compute locations of some characteristic points (keypoints) and describe them somehow. Even then, however, the resulting represenation is a low level one and still incomprehensible for humans. Defining robot's behaviour basing on such data might prove cumbersome, for there are too many details to consider. All of the above is reffered to as a semantic gap. It is defined by Tsai, with respect to the Content Based Image Retrieval or \textit{CBIR}, as \emph{``the gap between the extracted and indexed low-level features by computers and the high-level concepts (or semantics) of userâ€™s queries''}. Suppose a text document can be created from an image. If this is the case, then the document can be converted into a BoW model. Since the latter contains information about the semantic significance of the image an impact of the semantic gap can be reduced.
  
  As for the second question, there is a quite well established pipeline that enables creation of text documents from images. The steps of the BoW methodology are as follows:
  \begin{itemize}
   \item Keypoint detection --- keypoints are local interest points or regions. They are usually computed in such a way so as to provide scale and location invariance. Rigid transformation and illumination invariance would be desired but it is somewhat harder to achieve
   \item Keypoint description --- each keypoint have to be described in a manner that distinguish the particular keypoint in some way.
   \item Vector quantization --- clustering algorithms are used to find regions in the high dimensional space of keypoint descriptors. When the clusters are found, each described keypoint can be assigned to a corresponding cluster. Then, an image can be represented by the numbers of clusters, to which the image's keypoints were assigned. The numbers themselves are called \emph{`visual words'}.
  \end{itemize}
  
\section{BoW in Computer Vision}

  The Bag of Words image representation has been extensively used in the areas of scene \cite{csurka2004visual,fei2005bayesian, tsai2012bag} and object categorization \cite{zhangcategory} as well as CBIR \cite{li2010investigating,toldo2009bag} yielding state-of-the-art results. Advantages of this model are simplicity, computational efficiency and at least partial invariance to affine transformation, occlusion and lighting conditions. 
  
  The Bag of Words is a type of intermediate representation. Therefore it is not sufficient to compute a BoW model of an image in order to predict its category. Additional operations are essential if the model is to be used in one of the enumerated fields.
  
  \subsection{Content Based Image Retrieval}
  
  One of the most notorious use-case of \emph{CBIR} is to search a database in order to find an object fulfilling certain conditions. As \cite{toldo2009bag} outlines, several criteria have to be met for the task to be performed efficiently. Firstly, all entries should be indexed in a concise way. Secondly, some (dis)similarity measure should be provided. Finally, an efficient search algorithm should be available.  
    
  There are numerous methods suitable for computation of objetcs' signatures. Many of them can be used in the indexing step. All the methods were divided into three general categories in \cite{toldo2009bag}, specifically: feature based methods, graph based methods and other methods. 
   
  Feature based methods can be either global or local. The former takes the form of a single vector or a point in a $d$ dimensional space --- the similarity measure resulting in computing point-wise distance in that space. The latter gives multiple such points for each object, rendering the computation of a similarity measure slightly more complicated. The global features takes forms of the models' volume, their volume or mass distribution. Others might incorporate the global features' distribtution --- one conceivable approach would be to compute global feature's distribution and summarize them into a histogram. These features have the advantage of being easy to compute and straightforward to implement. However, they are insensitive to any local shape variations, thus being ill suited for details comparisions. On the other hand, they might be exploited in the preprocessing of the data with more sophisticated methods being used afterwards. Partial matching is not possible, since the global features do not encode any relations between the parts of the objects.
   
  As for the local features, \cite{toldo2009bag} describes only the features that describe neighbourhood of the points being on the boundary of a shape. While that might generally be true, different points might be considered as well. Moreover, the authors state that the local features based methods are inefficien and indexing is rather complex. A Bag of Words based approach, described in \cite{li2010investigating} has no such drawbacks.
   
  Even tough BoW is a local features based method, it can be regarded as a feature distribution method. It is based on visual words, clearly being local features. When all the image's visual words are summarized into a histogram a distribution is created. Being similar to a point in a multi-dimensional space, it is similar to a global feature. Consequently, similar methods apply, with the distinction being that the histogram is rather a vector then a point. Thus metrics well-suited to vector comparision, such as a cosine distance, can be used.
  
  \subsection{Scene Categorization}
  
  One of the first works employing the BoW for the purpouse of scene categorization is \cite{csurka2004visual}. The authors suggested a general framework. What is more, algorthims performing each main pipeline's step were proposed and evaluated as well as a codebook construction method was developed. A \emph{Harris Affine Detector} was used for feature extraction and \emph{SIFT} for the feature descripton. The visual vocabulary has been generated by clustering only a limited number of keypoints from each category. After summarizing each image in the traning set with a histogram of visual words, supervised learning was used to train two classifiers: Na\`ive Bayes and a Support Vector Machine. As expected the Na\`ive Bayes resulted in lower accuracy than SVM (72 vs. 85\% on 7 categories). An analysis on the number of centroids in the clustering step depicts that the greater size of the visual vocabulary migh increase accuracy. 
  
  Li \emph{et al} further refined the above approach by examining several keypoint detectors and descriptors. The main contribution of this work is, however, the development of a genuine classification algorithm based on probabilistic graphical models. Accuracy of 76\% on a large 13 category dataset was achieved.
  
  \subsection{Object Categorization}
  
  In many cases object categorization might be addressed as a scene categorization problem. There are following differences: (1) an object should be localized on an image (i.e. its bounding box has to be found) and (2) in object categorization information about the scene type might be used. The second case is symmetric, for in the scene categorization information about the objects present in the scene can be utilized as well. Having said that, it is possible to consider an object categorization problem where images of singled-out objects are provided --- e.g. a single object covers the majority of an image's area. Such an approach was exploited a recent work by Zhang \emph{et al} \cite{zhangcategory}. 
  

\section{BoW based classification}

	The Bag of Words intermediate image represenation provides a concise way of summarizing images. It is invariant to affaine transformations, partial occlusion and, in some extent, to changing lighting conditions. What is more, it is easy to implement and computationally efficient. The efficiency can be enhanced even further with gpu based implementations as shown in \cite{van2011empowering}. In order to incorporate BoW into an object classification framework one has to feed the resulting histograms into a classification algorithms. Either generative or discriminative methods can be used. Below the main steps of BoW classification processing pipeline will be disscussed.

\subsection{Detection}

	Characteristic point detection is the first step in any Bag of Words framework. Numerous detection methods have been developed, but choosing the right one for the particular case might prove tricky. A good overview of various mechanisms is available in \cite{tsai2012bag}. The most common detectors make use of a Harris corner detector or image's first or second derivatives. A technique taking advantage of the Harris corner detector is for example a Harris-Laplace detector --- the Harris function is scale adapted and its outcome is a subject to a Laplacian-of-Gaussian operator, which selects relevant points in the scale space. Images' regions' 2\textsuperscript{nd} derviatives --- namely the regions' Hessians --- can be combined with a LoG operator. This combination allows selection of points significant in the two spaces: the scale space and the Hessian's determinant space. The latter entails the speed at which pixel intensities change in the neighbourhood of a point.	
	
	A number of more complicated recipes for salient region localization have been developed and implemented. These include include Scale Invariant Feature Transform \cite{sift_keypoint}, SUSAN \cite{susan_keypoint} and Intrinistic Shapes Siganutes \cite{iss_keypoint}. The majority of keypoint detection formulas is being develoepd for the 2d domain. A number of them have been adapted to 3D, however. A comparative evaluation of detection algorithm available in PCL can be find in \cite{pcl_keypoint_comparision}. Another comprehensive study is \cite{3d_keypoint_eval}.
	
	All these formulas, called sparse feature detectors, resort to selection of maxima in specific state spaces. An entierly different scheme is to use a dense feature detector. This particular form requires users to specify a uniformly sampled grid from which points are taken. Dense detectors have an advantage of taking points from slow changing regions. A sparse detector might be unable to summarize a slow changing region such as clouds, sky or ocean. Li \emph{et al} showed that the dense detectors outperforms the sparse ones.
	
	
	
\subsection{Description}

	Localization of a salient point is not enough. If a keypoint is to be affaine transform invariant, it has to be described in more general terms then it's coordinates. Such description, usually coordinates in a multi-dimensional space, is provided by specialized algorithms. 128-dimensional SIFT \cite{sift_features} is a 3D histogram of gradient locations and orientetions. It is the most often extracted and well as the most effective descriptors \cite{tsai2012bag}. Other methods include various color descriptors, binary 512-dimensional GIST \cite{ponce2011cv}. There are techniques designed for 3D exclusively. Among them one can find Persistent Point Features Histogram \cite{pfh_rusu2008} and its faster alternative Fast Point Feature Histogram \cite{fpfh_rusu2009}, both implemented in PCL.

\subsection{Vector Quantization}
\subsection{Classification}

\bibliography{article}

\end{document}

